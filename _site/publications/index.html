

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Publications - Chao Liang</title>



<meta name="description" content="Your Name’s academic portfolio">







  <link rel="canonical" href="http://localhost:4000/publications/">





  

  






  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Chao Liang",
      "url" : "http://localhost:4000",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->

<!-- Open Graph protocol data (https://ogp.me/), used by social media -->
<meta property="og:locale" content="en-CN">
<meta property="og:site_name" content="Chao Liang"> 
<meta property="og:title" content="Publications">



  <meta property="og:url" content="http://localhost:4000/publications/">





<!-- end Open Graph protocol -->

<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Chao Liang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<!-- Support for Academicons -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.css"/>

<!-- favicon from https://commons.wikimedia.org/wiki/File:OOjs_UI_icon_academic-progressive.svg -->
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:4000/images/apple-touch-icon-180x180.png"/>
<link rel="icon" type="image/svg+xml" href="http://localhost:4000/images/favicon.svg"/>
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-32x32.png" sizes="32x32"/>
<link rel="icon" type="image/png" href="http://localhost:4000/images/favicon-192x192.png" sizes="192x192"/>
<link rel="manifest" href="http://localhost:4000/images/manifest.json"/>
<link rel="icon" href="/images/favicon.ico"/>
<meta name="theme-color" content="#ffffff"/>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg persist"><a href="http://localhost:4000/">Chao Liang</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/research/">Research</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/files/cv_Liang_en.pdf">CV</a></li>
          
          <li id="theme-toggle" class="masthead__menu-item persist tail">
            <a role="button" aria-labelledby="theme-icon"><i id="theme-icon" class="fa-solid fa-sun" aria-hidden="true" title="toggle theme"></i></a>
          </li>
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="http://localhost:4000/images/profile.png" class="author__avatar" alt="Chao Liang"  fetchpriority="high" />
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Chao Liang</h3>
    
    <p class="author__bio">梁超</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      <!-- Font Awesome icons / Biographic information  -->
      
        <li class="author__desktop"><i class="fas fa-fw fa-location-dot icon-pad-right" aria-hidden="true"></i>WuHan,China</li>
      
      
        <li class="author__desktop"><i class="fas fa-fw fa-building-columns icon-pad-right" aria-hidden="true"></i>Wuhan University</li>
      
      
      
        <li><a href="mailto:cliang@whu.edu.cn"><i class="fas fa-fw fa-envelope icon-pad-right" aria-hidden="true"></i>Email</a></li>
      

      <!-- Font Awesome and Academicons icons / Academic websites -->
      
            
      
        <li><a href="https://scholar.google.com/citations?user=JQpmKD0AAAAJ&hl=zh-CN"><i class="ai ai-google-scholar ai-fw icon-pad-right"></i>Google Scholar</a></li>
      
      
      
      
                              
      
      
      

      <!-- Font Awesome icons / Repositories and software development -->
      
            
            
      
            
            

      <!-- Font Awesome icons / Social media -->
              
      
      
            
      
                  
                  
      
            
            
            
      
            
                  
            
      
            
            
      
              
      
                      
      
      
            
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Publications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Publications
</h1>
          
        
        
            
        </header>
      

      <section class="page__content" itemprop="text">
        <style>
  .page__title {
    display: none !important;
  }
  .publications-page .publications-header {
    color: #4A90E2;
    font-size: 2.5rem;
    font-weight: 300;
    margin-bottom: 0.2em;
    margin-top: 0;
    line-height: 1.4;
  }
  .publication-year-section {
    margin-bottom: 1rem;
  }
  .publication-year-title {
    font-size: 0.8rem;
    font-weight: 400;
    margin-bottom: 0.2rem;
    color: #333;
    border-bottom: 2px solid #4A90E2;
    padding-bottom: 0.1rem;
    line-height: 0.8;
  }
  .publications-list {
    margin-left: 0;
  }
  .publication-item {
    margin-bottom: 0rem;
    padding-bottom: 0rem;
    border-bottom: none;
    list-style: none;
    position: relative;
    padding-left: 20px;
    line-height: 1.3;
  }
  .publication-item::before {
    content: "•";
    position: absolute;
    left: 0;
    top: 0;
    color: #4A90E2;
    font-weight: bold;
    font-size: 1.2em;
  }
  .publication-authors {
    font-size: 1rem;
    margin-bottom: 0.5rem;
    color: #555;
    line-height: 1.4;
  }
  .publication-title {
    font-size: 1.1rem;
    margin-bottom: 0.5rem;
    line-height: 0.2;
  }
  .publication-title strong {
    font-weight: 400;
    color: #333;
    line-height: 1.2;
  }
  .presentation-type {
    color: #4A90E2;
    font-weight: 500;
    margin-left: 0.3rem;
  }
  .pdf-link {
    color: #4A90E2;
    text-decoration: none;
    font-weight: normal;
    margin-left: 0.5rem;
  }
  .pdf-link:hover {
    text-decoration: underline;
  }
  .publication-venue {
    font-size: 0.95rem;
    color: #666;
    font-style: italic;
    margin-bottom: 0.1rem;
  }
  .publications-list {
    padding-left: 0;
  }
  .publication-item {
    list-style: none;
  }
  .publication-links {
    margin: 10px 0;
  }
  .publication-links a {
    text-decoration: none;
    color: #4A90E2;
    margin-right: 10px;
  }
  .publication-links a:hover {
    text-decoration: underline;
  }
  .bibtex-content {
    display: none;
    background: #f5f5f5;
    padding: 15px;
    margin-top: 10px;
    border-radius: 5px;
    border: 1px solid #ddd;
    font-family: 'Courier New', monospace;
    font-size: 0.9em;
    line-height: 1.4;
  }
  .bibtex-content pre {
    margin: 0;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
  // 处理 BibTeX 显示
  const bibtexLinks = document.querySelectorAll('.show-bibtex');
  bibtexLinks.forEach(function(link) {
    link.addEventListener('click', function(e) {
      e.preventDefault();
      const bibtexContent = this.getAttribute('data-bibtex');
      const bibtexDiv = this.parentNode.parentNode.querySelector('.bibtex-content');
      const pre = bibtexDiv.querySelector('pre');
      
      if (bibtexDiv.style.display === 'none' || bibtexDiv.style.display === '') {
        pre.textContent = bibtexContent;
        bibtexDiv.style.display = 'block';
        this.textContent = '[Hide BibTeX]';
      } else {
        bibtexDiv.style.display = 'none';
        this.textContent = '[BibTeX]';
      }
    });
  });
});
</script>



<div class="publications-page">
  <h1 class="publications-header">Publications</h1>
  
      <div class="wordwrap">You can also find my articles on <a href="https://scholar.google.com/citations?user=JQpmKD0AAAAJ&hl=zh-CN">my Google Scholar profile</a>.</div>
  
  
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2025</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Link-based Contrastive Learning for One-Shot Unsupervised Domain Adaptation</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Yue Zhang, Mingyue Bin, Yuyang Zhang, Zhongyuan Wang, Zhen Han, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2025
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://xplorestaging.ieee.org/document/11095144" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@INPROCEEDINGS{11095144, author={Zhang, Yue and Bin, Mingyue and Zhang, Yuyang and Wang, Zhongyuan and Han, Zhen and Liang, Chao}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, title={Link-based Contrastive Learning for One-Shot Unsupervised Domain Adaptation}, year={2025}, volume={}, number={}, pages={4916-4926}, keywords={Representation learning;Access control;Computer vision;Limiting;Face recognition;Surveillance;Contrastive learning;Benchmark testing;Public security;Faces;single-shot;unsupervised domain adaptation;pseudo links;contrastive learning}, doi={10.1109/CVPR52734.2025.00463}}">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Rethinking the Adversarial Robustness of Multi-Exit Neural Networks in an Attack-Defense Game</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Keyizhi Xu, Chi Zhang, Zhan Chen, Zhongyuan Wang, Chunxia Xiao, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2025
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://xplorestaging.ieee.org/document/11092781" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@INPROCEEDINGS{11092781, author={Xu, Keyizhi and Zhang, Chi and Chen, Zhan and Wang, Zhongyuan and Xiao, Chunxia and Liang, Chao}, booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, title={Rethinking the Adversarial Robustness of Multi-Exit Neural Networks in an Attack-Defense Game}, year={2025}, volume={}, number={}, pages={10265-10274}, keywords={Fault diagnosis;Computer vision;Neural networks;Games;Computer architecture;Nash equilibrium;Robustness;Pattern recognition;adversarial robustness;game theory;multi-exit networks}, doi={10.1109/CVPR52734.2025.00960}} ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Friend-sQA: A New Large-Scale Deep Video Understanding Dataset with Fine-grained Topic Categorization for Story Videos</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Zhengqian Wu, Ruizhe Li, Zijun Xu, Zhongyuan Wang, Chunxia Xiao, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  AAAI Conference on Artificial Intelligence (AAAI), 2025
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32920" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@article{Wu_Li_Xu_Wang_Xiao_Liang_2025, title={FriendsQA: A New Large-Scale Deep Video Understanding Dataset with Fine-grained Topic Categorization for Story Videos}, volume={39}, url={https://ojs.aaai.org/index.php/AAAI/article/view/32920}, DOI={10.1609/aaai.v39i8.32920}, abstractNote={Video question answering (VideoQA) aims to answer natural language questions according to the given videos. Although existing models perform well in the factoid VideoQA task, they still face challenges in deep video understanding (DVU) task, which focuses on story videos. Compared to factoid videos, the most significant feature of story videos is storylines, which are composed of complex interactions and long-range evolvement of core story topics including characters, actions and locations. Understanding these topics requires models to possess DVU capability. However, existing DVU datasets rarely organize questions according to these story topics, making them difficult to comprehensively assess VideoQA models’ DVU capability of complex storylines. Additionally, the question quantity and video length of these dataset are limited by high labor costs of handcrafted dataset building method. In this paper, we devise a large language model based multi-agent collaboration framework, StoryMind, to automatically generate a new large-scale DVU dataset. The dataset, FriendsQA, derived from the renowned sitcom Friends with an average episode length of 1,358 seconds, contains 44.6K questions evenly distributed across 14 fine-grained topics. Finally, We conduct comprehensive experiments on 10 state-of-the-art VideoQA models using the FriendsQA dataset.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wu, Zhengqian and Li, Ruizhe and Xu, Zijun and Wang, Zhongyuan and Xiao, Chunxia and Liang, Chao}, year={2025}, month={Apr.}, pages={8523-8531} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Who, What and Where: Composite-Semantics Instance Search for Story Videos</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Jiahao Guo, Ankang Lu, Zhengqian Wu, Zhongyuan Wang, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Transactions on Image Processing (TIP), 2025
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://xplorestaging.ieee.org/document/10899766" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@ARTICLE{10899766, author={Guo, Jiahao and Lu, Ankang and Wu, Zhengqian and Wang, Zhongyuan and Liang, Chao}, journal={IEEE Transactions on Image Processing}, title={Who, What, and Where: Composite-Semantics Instance Search for Story Videos}, year={2025}, volume={34}, number={}, pages={1412-1426}, keywords={Videos;Semantics;Correlation;TV;Feature extraction;Chaos;Training;Support vector machines;Search problems;NIST;Who-what-where;instance search;video structure aware;partial decomposition}, doi={10.1109/TIP.2025.3542272}} ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Floor Plan Restoration: A Multimodal Method Under One Second</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Tao Wen, You-Ming Fu*, Chun-Xia Xiao, Hai-Ming Xiang, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Transactions on Visualization and Computer Graphics (TVCG), 2025
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://xplorestaging.ieee.org/document/10876804" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@ARTICLE{10876804, author={Wen, Tao and Fu, You-Ming and Xiao, Chun-Xia and Xiang, Hai-Ming and Liang, Chao}, journal={IEEE Transactions on Visualization and Computer Graphics}, title={Floor Plan Restoration: A Multimodal Method Under One Second}, year={2025}, volume={31}, number={10}, pages={7107-7120}, keywords={Floors;Semantics;Layout;Image restoration;Vectors;Accuracy;Semantic segmentation;Annotations;Principal component analysis;Optical character recognition;Floor plan parsing;semantic segmentation;image vectorization}, doi={10.1109/TVCG.2025.3539497}} ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2024</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>A Survey on Rank Aggregation</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Siyi Wang, Qi Deng, Shiwei Feng, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  International Joint Conference on Artificial Intelligence (IJCAI), 2024
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://www.ijcai.org/proceedings/2024/915" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@inproceedings{ijcai2024p915, title     = {A Survey on Rank Aggregation}, author    = {Wang, Siyi and Deng, Qi and Feng, Shiwei and Zhang, Hong and Liang, Chao}, booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}}, publisher = {International Joint Conferences on Artificial Intelligence Organization}, editor    = {Kate Larson}, pages     = {8281--8289}, year      = {2024}, month     = {8}, note      = {Survey Track}, doi       = {10.24963/ijcai.2024/915}, url       = {https://doi.org/10.24963/ijcai.2024/915}, } ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>QI-IRA Quantum-Inspired Interactive Ranking Aggregation for Person Re-identification</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Chunyu Hu, Hong Zhang, <b>Chao Liang</b>*, Hao Huang</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  AAAI Conference on Artificial Intelligence (AAAI), 2024
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/27993" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@article{Hu_Zhang_Liang_Huang_2024, title={QI-IRA: Quantum-Inspired Interactive Ranking Aggregation for Person Re-identification}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/27993}, DOI={10.1609/aaai.v38i3.27993}, abstractNote={Ranking aggregation (RA), the process of aggregating multiple rankings derived from multiple search strategies, has been proved effective in person re-identification (re-ID) because of a single re-ID method can not always achieve consistent superiority for different scenarios. Existing RA research mainly focus on unsupervised and fully-supervised methods. The former lack external supervision to optimize performance, while the latter are costly because of expensive labeling effort required for training. To address the above challenges, this paper proposes a quantum-inspired interactive ranking aggregation (QI-IRA) method, which (1) utilizes quantum theory to interpret and model the generation and aggregation of multiple basic rankings, (2) approximates or even exceeds the performance of fully-supervised RA methods with much less labeling cost, even as low as only two feedbacks per query on Market1501, MARS and DukeMTMC-VideoReID datasets. Comparative experiments conducted on six public re-ID datasets validate the superiority of the proposed QI-IRA method over existing unsupervised, interactive, and fully-supervised RA approaches.}, number={3}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Hu, Chunyu and Zhang, Hong and Liang, Chao and Huang, Hao}, year={2024}, month={Mar.}, pages={2202-2210} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Toward Robust Adversarial Purification for Face Recognition under Intensity-Unknown Attacks</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Keyizhi Xu, Zhan Chen, Zhongyuan Wang, Chunxia Xiao, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Transactions on Information Forensics & Security (TIFS), 2024
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://ieeexplore.ieee.org/document/10704749" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@ARTICLE{10704749, author={Xu, Keyizhi and Chen, Zhan and Wang, Zhongyuan and Xiao, Chunxia and Liang, Chao}, journal={IEEE Transactions on Information Forensics and Security}, title={Toward Robust Adversarial Purification for Face Recognition Under Intensity-Unknown Attacks}, year={2024}, volume={19}, number={}, pages={9550-9565}, keywords={Purification;Face recognition;Training;Perturbation methods;Image resolution;Robustness;Noise;Multimedia communication;Image reconstruction;Feature extraction;Adversarial defense;intensity-unknown attacks;face recognition}, doi={10.1109/TIFS.2024.3473293}} ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2023</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>A Hierarchical Deep Video Understanding Method with Shot-Based Instance Search and Large Language Model</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Ruizhe Li, Jiahao Guo, Mingxi Li, Zhengqian Wu, <b>Chao Liang</b>*</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  ACM Multimedia (MM), 2023
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://dlnext.acm.org/doi/abs/10.1145/3581783.3612838" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@inproceedings{10.1145/3581783.3612838, author = {Li, Ruizhe and Guo, Jiahao and Li, Mingxi and Wu, Zhengqian and Liang, Chao}, title = {A Hierarchical Deep Video Understanding Method with Shot-Based Instance Search and Large Language Model}, year = {2023}, isbn = {9798400701085}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3581783.3612838}, doi = {10.1145/3581783.3612838}, abstract = {Deep video understanding (DVU) is often considered a challenge due to the aim of interpreting a video with storyline, which is designed to solve two levels of problems: predicting the human interaction in scene-level and identifying the relationship between two entities in movie-level. Based on our understanding of the movie characteristics and analysis of DVU tasks, in this paper, we propose a four-stage method to solve the task, which includes video structuring, shot based instance search, interaction &amp; relation prediction and shot-scene summary &amp; Question Answering (QA) with ChatGPT. In these four stages, shot based instance search allows accurate identification and tracking of characters at an appropriate video granularity. Using ChatGPT in QA, on the one hand, can narrow the answer space, on the other hand, with the help of the powerful text understanding ability, ChatGPT can help us answer the questions by giving background knowledge. We rank first in movie-level group 2 and scene-level group 1, second in movie-level group 1 and scene-level group 2 in ACM MM 2023 Grand Challenge.}, booktitle = {Proceedings of the 31st ACM International Conference on Multimedia}, pages = {9425-9429}, numpages = {5}, keywords = {instance search, multi-modal feature, video understanding}, location = {Ottawa ON, Canada}, series = {MM &#39;23} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Confidence-Aware Active Feedback for Interactive Instance Search</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Yue Zhang, <b>Chao Liang</b>*, Longxiang Jiang</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Transactions on Multimedia (TMM), 2023
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://xplorestaging.ieee.org/document/9931993" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@ARTICLE{9931993, author={Zhang, Yue and Liang, Chao and Jiang, Longxiang}, journal={IEEE Transactions on Multimedia}, title={Confidence-Aware Active Feedback for Interactive Instance Search}, year={2023}, volume={25}, number={}, pages={7173-7184}, keywords={Task analysis;Radio frequency;Manifolds;Probes;Training;Optimization;Search problems;Active learning;Interactive instance search}, doi={10.1109/TMM.2022.3217965}} ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Person-Action Instance Search in Story Videos: An Experimental Study</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Yanrui Niu, <b>Chao Liang</b>*, Ankang Lu, Baojin Huang, Zhongyuan Wang, Jiahao Guo</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  ACM Transactions on Information Systems (TOIS), 2023
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://dl.acm.org/doi/abs/10.1145/3617892" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@article{10.1145/3617892, author = {Niu, Yanrui and Liang, Chao and Lu, Ankang and Huang, Baojin and Wang, Zhongyuan and Guo, Jiahao}, title = {Person-action Instance Search in Story Videos: An Experimental Study}, year = {2023}, issue_date = {March 2024}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {42}, number = {2}, issn = {1046-8188}, url = {https://doi.org/10.1145/3617892}, doi = {10.1145/3617892}, abstract = {Person-Action instance search (P-A INS) aims to retrieve the instances of a specific person doing a specific action, which appears in the 2019–2021 INS tasks of the world-famous TREC Video Retrieval Evaluation (TRECVID). Most of the top-ranking solutions can be summarized with a Division-Fusion-Optimization (DFO) framework, in which person and action recognition scores are obtained separately, then fused, and, optionally, further optimized to generate the final ranking. However, TRECVID only evaluates the final ranking results, ignoring the effects of intermediate steps and their implementation methods. We argue that conducting the fine-grained evaluations of intermediate steps of DFO framework will (1) provide a quantitative analysis of the different methods’ performance in intermediate steps; (2) find out better design choices that contribute to improving retrieval performance; and (3) inspire new ideas for future research from the limitation analysis of current techniques. Particularly, we propose an indirect evaluation method motivated by the leave-one-out strategy, which finds an optimal solution surpassing the champion teams in 2020–2021 INS tasks. Moreover, to validate the generalizability and robustness of the proposed solution under various scenarios, we specifically construct a new large-scale P-A INS dataset and conduct comparative experiments with both the leading NIST TRECVID INS solution and the state-of-the-art P-A INS method. Finally, we discuss the limitations of our evaluation work and suggest future research directions.}, journal = {ACM Trans. Inf. Syst.}, month = nov, articleno = {46}, numpages = {34}, keywords = {Movie video, composite concepts, person-action instance search} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
    
    
  
    
    
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2020</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Correlation Discrepancy Insight Network for Video Re-identification</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Weijian Ruan, <b>Chao Liang</b>*, Yi Yu, Zheng Wang, Wu Liu, Jun Chen, Jiayi Ma</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  ACM Transactions on Multimedia Computing, Communication and Applications (TOMM), 2020
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://dl.acm.org/doi/10.1145/3402666" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@article{10.1145/3402666, author = {Ruan, Weijian and Liang, Chao and Yu, Yi and Wang, Zheng and Liu, Wu and Chen, Jun and Ma, Jiayi}, title = {Correlation Discrepancy Insight Network for Video Re-identification}, year = {2020}, issue_date = {November 2020}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {16}, number = {4}, issn = {1551-6857}, url = {https://doi.org/10.1145/3402666}, doi = {10.1145/3402666}, abstract = {Video-based person re-identification (ReID) aims at re-identifying a specified person sequence from videos that were captured by disjoint cameras. Most existing works on this task ignore the quality discrepancy across frames by using all video frames to develop a ReID method. Additionally, they adopt only the person self-characteristic as the representation, which cannot adapt to cross-camera variation effectively. To that end, we propose a novel correlation discrepancy insight network for video-based person ReID, which consists of an unsupervised correlation insight model (CIM) for video purification and a discrepancy description network (DDN) for person representation. Concretely, CIM is constructed by using kernelized correlation filters to encode person half-parts, which evaluates the frame quality by the cross correlation across frames for selecting discriminative video fragments. Furthermore, DDN exploits the selected video fragments to generate a discrepancy descriptor using a compression network, which aims at employing the discrepancies with other persons’ to facilitate the representation of the target person rather than only using the self-characteristic. Due to the advantage in handling cross-domain variation, the discrepancy descriptor is expected to provide a new pattern for the object representation in cross-camera tasks. Experimental results on three public benchmarks demonstrate that the proposed method outperforms several state-of-the-art methods.}, journal = {ACM Trans. Multimedia Comput. Commun. Appl.}, month = dec, articleno = {120}, numpages = {21}, keywords = {Video re-identification, correlation insight, cross-domain variation, discrepancy description network} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
    
    
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2018</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Video-based Person Re-identification via Self Paced Weighting</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Wenjun Huang, <b>Chao Liang</b>*, Yi Yu, Zheng Wang, Weijian Ruan, Ruimin Hu</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  AAAI Conference on Artificial Intelligence (AAAI), 2018
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/11857" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@article{Huang_Liang_Yu_Wang_Ruan_Hu_2018, title={Video-Based Person Re-Identification via Self Paced Weighting}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11857}, DOI={10.1609/aaai.v32i1.11857}, abstractNote={ &amp;lt;p&amp;gt; Person re-identification (re-id) is a fundamental technique to associate various person images, captured by differentsurveillance cameras, to the same person. Compared to the single image based person re-id methods, video-based personre-id has attracted widespread attentions because extra space-time information and more appearance cues that can beused to greatly improve the matching performance. However, most existing video-based person re-id methods equally treatall video frames, ignoring their quality discrepancy caused by object occlusion and motions, which is a common phenomenonin real surveillance scenario. Based on this finding, we propose a novel video-based person re-id method via self paced weighting (SPW). Firstly, we propose a self paced outlier detection method to evaluate the noise degree of video sub sequences. Thereafter, a weighted multi-pair distance metric learning approach is adopted to measure the distance of two person image sequences. Experimental results on two public datasets demonstrate the superiority of the proposed method over current state-of-the-art work. &amp;lt;/p&amp;gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Huang, Wenjun and Liang, Chao and Yu, Yi and Wang, Zheng and Ruan, Weijian and Hu, Ruimin}, year={2018}, month={Apr.} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
    
    
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2016</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Person Reidentification via Ranking Aggregation of Similarity Pulling and Dissimilarity Pushing</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Mang Ye, <b>Chao Liang</b>*, Yi Yu, Zheng Wang, Qingming Leng, Chunxia Xiao, Jun Chen, Ruimin Hu</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  IEEE Transactions on Multimedia (TMM), 2016
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://xplorestaging.ieee.org/document/7557057" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@ARTICLE{7557057, author={Ye, Mang and Liang, Chao and Yu, Yi and Wang, Zheng and Leng, Qingming and Xiao, Chunxia and Chen, Jun and Hu, Ruimin}, journal={IEEE Transactions on Multimedia}, title={Person Reidentification via Ranking Aggregation of Similarity Pulling and Dissimilarity Pushing}, year={2016}, volume={18}, number={12}, pages={2553-2566}, keywords={Probes;Optimization methods;Cameras;Feature extraction;Image retrieval;Multimedia communication;Person reidentification;ranking aggregation;similarity and dissimilarity}, doi={10.1109/TMM.2016.2605058}} ">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
    
    
      <div class="publication-year-section">
        <h2 class="publication-year-title">2015</h2>
        <div class="publications-list">
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>A Unsupervised Person Re-identification Method Using Model Based Representation and Ranking</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors"><b>Chao Liang</b>, Bingyue Huang, Ruimin Hu, Chunjie Zhang, Xiao-Yuan Jing, Jing Xiao</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  ACM Multimedia (MM), 2015
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://dl.acm.org/doi/10.1145/2733373.2807399" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@inproceedings{10.1145/2733373.2807399, author = {Liang, Chao and Huang, Binyue and Hu, Ruimin and Zhang, Chunjie and Jing, Xiaoyuan and Xiao, Jing}, title = {A Unsupervised Person Re-identification Method Using Model Based Representation and Ranking}, year = {2015}, isbn = {9781450334594}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2733373.2807399}, doi = {10.1145/2733373.2807399}, abstract = {As a core technique supporting the multi-camera tracking task, person re-identification attracts increasing research interests in both academic and industrial communities. Its aim is to match individuals across a group of spatially non-overlapping surveillance cameras, which are usually interfered by various imaging conditions and object motions. Current methods mainly focus on robust feature representation and accurate distance measure, where intensive computations and expensive training samples prohibit their practical applications. To address the above problems, this paper proposes a new unsupervised person re-identification method featured by its competitive accuracy and high efficiency. Both merits stem from model based person image representation and ranking, with which, merely 4-dimension pixel-level features can achieve over 20% matching rate at Rank 1 on the challenging VIPeR dataset.}, booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia}, pages = {771-774}, numpages = {4}, keywords = {person re-identification, model representation and ranking}, location = {Brisbane, Australia}, series = {MM &#39;15} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
            <div class="publication-item">
              <!-- 标题 -->
              <div class="publication-title">
                <strong>Ranking Optimization for Person Re-identification via Similarity and Dissimilarity</strong>
                
              </div>
              
              <!-- 作者 -->
              <div class="publication-authors">Mang Ye, <b>Chao Liang</b>*, Zheng Wang, Qingming Leng, Jun Chen</div>
              
              <!-- 期刊/会议 -->
              <div class="publication-venue">
                
                  ACM Multimedia (MM), 2015
                
              </div>
              
              <!-- 链接 -->
              <div class="publication-links">
                
                  [<a href="https://dl.acm.org/doi/10.1145/2733373.2806326" target="_blank" class="pdf-link">Paper</a>]
                
                
                  [<a href="#" class="show-bibtex" data-bibtex="@inproceedings{10.1145/2733373.2806326, author = {Ye, Mang and Liang, Chao and Wang, Zheng and Leng, Qingming and Chen, Jun}, title = {Ranking Optimization for Person Re-identification via Similarity and Dissimilarity}, year = {2015}, isbn = {9781450334594}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/2733373.2806326}, doi = {10.1145/2733373.2806326}, abstract = {Person re-identification is a key technique to match different persons observed in non-overlapping camera views.Many researchers treat it as a special object retrieval problem, where ranking optimization plays an important role. Existing ranking optimization methods utilize the similarity relationship between the probe and gallery images to optimize the original ranking list in which dissimilarity relationship is seldomly investigated. In this paper, we propose to use both similarity and dissimilarity cues in a ranking optimization framework for person re-identification. Its core idea is based on the phenomenon that the true match should not only be similar to the strong similar samples of the probe but also dissimilar to the strong dissimilar samples. Extensive experiments have shown the great superiority of the proposed ranking optimization method.}, booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia}, pages = {1239–1242}, numpages = {4}, keywords = {person re-identification, ranking optimization, similarity and dissimilarity}, location = {Brisbane, Australia}, series = {MM &#39;15} }">BibTeX</a>]
                
              </div>
              
              <!-- BibTeX 内容区域 -->
              <div class="bibtex-content" style="display: none; background: #f5f5f5; padding: 10px; margin-top: 10px; border-radius: 5px;">
                <pre style="margin: 0; white-space: pre-wrap; font-family: 'Courier New', monospace;"></pre>
              </div>
            </div>
          
        </div>
      </div>
    
  
</div>

        

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<a href="/sitemap/">Sitemap</a>

<!-- Support for MatJax -->
<script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>

<!-- Support for Plotly -->
<script defer src='https://cdnjs.cloudflare.com/ajax/libs/plotly.js/3.0.1/plotly.min.js'></script>

<!-- Support for Mermaid -->
<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
    mermaid.initialize({startOnLoad:true, theme:'default'});
    await mermaid.run({querySelector:'code.language-mermaid'});
</script>

<!-- end custom footer snippets -->

        


<div class="page__footer-follow">
  <ul class="social-icons">
    
    
    
    
    <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">
  &copy; 2025 Chao Liang, Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.<br />
  Site last updated 2025-09-29
</div>

      </footer>
    </div>

    <script type="module" src="http://localhost:4000/assets/js/main.min.js"></script>








  </body>
</html>

